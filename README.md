# Improve Algorithmic Fairness in Recidivism Prediction

Algorithmic fairness has become an essential research topic due to the potential for machine learning algorithms to perpetuate existing biases and discrimination in society. Many algorithms have been shown to produce unfair
outcomes for certain groups, particularly those who have historically faced
discrimination. Therefore, it is important that the outcomes of Machine
Learning (ML) models are fair and do not lead to discrimination. Improving
algorithmic fairness involves developing methods to ensure that algorithms
are fair and unbiased in their decision-making processes.
One approach to improving algorithmic fairness is using data preprocessing
techniques such as data cleaning, sampling, and feature engineering. These
techniques aim to remove or mitigate any biases that may be present in the
data used to train the algorithm. Another approach involves modifying the
algorithm to incorporate fairness constraints, such as requiring the algorithm
to treat all groups equally.
In this thesis, we analyzed five different bias mitigation methods belonging
to different fairness-enhancing techniques and decide which method best
suits our dataset. The results show to great extent, the pre-processing technique works best for our dataset
